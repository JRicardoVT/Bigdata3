# Bigdata3
El objetivo del proyecto es demostrar cómo Spark puede procesar grandes volúmenes de información tanto en modo batch como en tiempo real, integrándose con Kafka para el flujo continuo de datos.
Para eso se creo 4 scripts en putty.exe usando el comando "nano":
- batch_processing.py
- batch_processing.py
- procesamiento_streaming.py
- spark_kafka_streaming.py
